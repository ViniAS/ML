{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435db6f4-e15b-4f9b-8cd0-b9b5dedd9e79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T12:35:11.661495Z",
     "start_time": "2024-03-26T12:35:11.606747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from sympy.vector import Laplacian\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8837d-5164-4be5-9997-fdd0a687f07e",
   "metadata": {},
   "source": [
    "**Instruções gerais:** Sua submissão <u>deve</u> conter: \n",
    "1. Um \"ipynb\" com seu código e as soluções dos problemas\n",
    "2. Uma versão pdf do ipynb\n",
    "\n",
    "Favor <u>não</u> enviar um .zip dos arquivos.\n",
    "Caso você opte por resolver as questões de \"papel e caneta\" em um editor de $\\LaTeX$ externo, o inclua no final da versão pdf do 'ipynb'--- submetendo um **<u>*único pdf*</u>**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58678575-bb0a-4ff2-85d2-f1ff100806f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trabalho de casa 03: Regressão logística e inferência Bayesiana aproximada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af932776-d827-4ee8-b749-c1aec78faadf",
   "metadata": {},
   "source": [
    "O pedaço de código abaixo carrega o banco de dados 'breast cancer' e adiciona uma coluna de bias. Além disse, ele o particiona em treino e teste.\n",
    "\n",
    "1. Implemente a estimativa de máximo a posteriori para um modelo de regressão logística com priori $\\mathcal{N}(0, c I)$ com $c=100$ usando esse banco de dados;\n",
    "2. Implemente a aproximação de Laplace para o mesmo modelo;\n",
    "3. Implemente uma aproximação variacional usando uma Gaussiana diagonal e o truque da reparametrização;\n",
    "4. Calcule a accuracy no teste para todas as opções acima --- no caso das 2 últimas, a prob predita é $\\int_\\theta p(y|x, \\theta) q(\\theta)$;\n",
    "5. Para cada uma das 3 técnicas, plote um gráfico com a distribuição das entropias para as predições corretas e erradas (separadamente), use a função kdeplot da biblioteca seaborn.\n",
    "6. Comente os resultados, incluindo uma comparação dos gráficos das entropias.\n",
    "\n",
    "Explique sua implementação também! \n",
    "\n",
    "Para facilitar sua vida: use PyTorch, Adam para otimizar (é uma variação SGD) com lr=0.001, use o banco de treino inteiro ao invés de minibatchces, use `binary_cross_entropy_with_logits` para implementar a -log verossimilhança, use `torch.autograd.functional` para calcular a Hessiana. Você pode usar as bibliotecas importadas na primeira célula à vontade. Verifique a documentação de `binary_cross_entropy_with_logits` para garantir que a sua priori está implementada corretamente, preservando as proporções devidas. Use 10000 amostras das aproximações para calcular suas predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21701c8f-7d43-450e-9541-38aec817de0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T12:35:11.682404Z",
     "start_time": "2024-03-26T12:35:11.669379Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data =  load_breast_cancer()\n",
    "N = len(data.data)\n",
    "Ntrain = int(np.ceil(N*0.6))\n",
    "perm = np.random.permutation(len(data.data))\n",
    "X = torch.tensor(data.data).float()\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), axis=1) \n",
    "y = torch.tensor(data.target).float()\n",
    "\n",
    "Xtrain, ytrain = X[perm[:Ntrain]], y[perm[:Ntrain]]\n",
    "Xtest, ytest = X[perm[Ntrain:]], y[perm[Ntrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0323d40-ffbc-4b85-a512-f0a979ed9e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T12:35:12.340391Z",
     "start_time": "2024-03-26T12:35:11.730872Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def maximum_a_posteriori(X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    priori_var = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    d = X.shape[1]\n",
    "\n",
    "    theta = torch.empty(d).normal_(std=priori_var ** 0.5).requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([theta], lr=learning_rate)\n",
    "    prior = MultivariateNormal(torch.zeros(d), priori_var * torch.eye(d))\n",
    "    for _ in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        log_likelihood = F.binary_cross_entropy_with_logits(X @ theta, y, reduction='sum')\n",
    "        log_prior = -prior.log_prob(theta)\n",
    "        loss = log_likelihood + log_prior\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if theta.grad.norm() < 1e-5:\n",
    "            print(\"stop\")\n",
    "            break\n",
    "        \n",
    "    return theta\n",
    "    \n",
    "\n",
    "theta = maximum_a_posteriori(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "937cd16c-3b19-4c54-9eb9-e559a2a458e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T12:40:48.203164Z",
     "start_time": "2024-03-26T12:40:47.485526Z"
    }
   },
   "outputs": [],
   "source": [
    "def laplacian(X: torch.Tensor, y: torch.Tensor):\n",
    "    mode = maximum_a_posteriori(X,y)\n",
    "    prior = MultivariateNormal(torch.zeros(X.shape[1]), 100 * torch.eye(X.shape[1]))\n",
    "    log_posterior = lambda theta: F.binary_cross_entropy_with_logits(X @ theta, y, reduction=\"sum\") - prior.log_prob(theta)\n",
    "    hessian_posterior = hessian(log_posterior, mode)\n",
    "    return MultivariateNormal(mode, torch.inverse(hessian_posterior))\n",
    "\n",
    "def laplacian_pred(X, laplacian_posterior):\n",
    "    samples = laplacian_posterior.sample((X.size(0),))\n",
    "    y_pred = X.mm(samples.t())\n",
    "    return y_pred\n",
    "\n",
    "laplacian_approx = laplacian(Xtrain,ytrain)\n",
    "y_pred_laplacian = laplacian_pred(X,laplacian_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d2a63-57b1-4f80-9b6c-3579d69ca1f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f268c40-0ccb-4420-9100-4cb669d98521",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercícios de \"papel e caneta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3e1ec-14cd-4bf8-bea4-20c06de871f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Derive a fórmula para a divergência KL entre duas distribuições Gaussianas univariadas, i.e., $D_\\text{KL}\\left(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)\\right)$;\n",
    "\n",
    "2. Suponha que $P$ é a família das distribuições categóricas com suporte em $\\{1,\\ldots, L\\}$. Qual $p \\in P$ possui maior entropia? \n",
    "\n",
    "3. Use a [desigualdade de Jensen](https://en.wikipedia.org/wiki/Jensen%27s_inequality) para mostrar que a divergência KL é não-negativa.\n",
    "\n",
    "> **Dica:** A desigualdade de Jensen afirma que, se $\\varphi$ é uma função convexa, então $\\varphi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\varphi(X)]$.\n",
    "\n",
    "4. Derive a aproximação de Laplace para a distribuição [Beta](https://en.wikipedia.org/wiki/Beta_distribution)($\\alpha, \\beta$). Mostre uma fórmula para valores genéricos $\\alpha,\\beta>1$ e a instancie para $\\alpha=\\beta=2$.\n",
    "\n",
    "5. Derive a posteriori para o modelo Bayesiano com verossimilhança [Categórica](https://en.wikipedia.org/wiki/Categorical_distribution) e priori [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution), i.e.:\n",
    "$$\n",
    "\\begin{align}{2}\n",
    "y_1,\\ldots, y_N &\\sim Cat(\\mathbf{\\theta})\\\\\n",
    "\\mathbf{\\theta} &\\sim Dirichlet(\\mathbf{\\alpha})\n",
    "\\end{align}\n",
    "$$\n",
    "onde $\\mathbf{\\theta}$ e $\\mathbf{\\alpha}$ são vetores $L$-dimensionais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb213bbab66bcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questão 1\n",
    "$$\n",
    "\\begin{align}\n",
    "D_\\text{KL}\\left(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)\\right) &= E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\log{\\frac{\\mathcal{N}(\\mu_1,\\sigma^2_1)}{\\mathcal{N}(\\mu_2,\\sigma^2_2)}}\\right]\\\\\n",
    "&=$E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\log{\\mathcal{N}(\\mu_1,\\sigma^2_1)}\\right]-E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\log{\\mathcal{N}(\\mu_2,\\sigma^2_2)}\\right]\\\\\n",
    "&=E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left\\{\\log\\left[{(2\\pi\\sigma^2_1)^{-\\frac{1}{2}}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)²\\right]}}\\right]\\right\\}-\n",
    "E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left\\{\\log\\left[{(2\\pi\\sigma^2_2)^{-\\frac{1}{2}}\\exp{\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)²\\right]}}\\right]\\right\\}\\\\\n",
    "&=-\\frac{1}{2}\\left\\{E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\log(2\\pi\\sigma_1^2)\\right]-E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\log(2\\pi\\sigma_2^2)\\right]+\n",
    "E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2\\right]-E_{\\mathcal{N}(\\mu_1,\\sigma_1)}\\left[\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2\\right]\\right\\}\n",
    "\\end{align}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
