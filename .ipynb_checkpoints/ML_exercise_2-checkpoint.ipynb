{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r5SEFD9D4KK"
   },
   "source": [
    "# Trabalho de casa 02: Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruções gerais:** Sua submissão deve conter: \n",
    "1. Um \"ipynb\" com seu código e as soluções dos problemas\n",
    "2. Uma versão pdf do ipynb\n",
    "\n",
    "Caso você opte por resolver as questões de \"papel e caneta\" em um editor de $\\LaTeX$ externo, o inclua no final da versão pdf do 'ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios computacionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC12OuanDqSJ"
   },
   "source": [
    "**Exercício 1.** Deixamos à sua disposição o dataset [\"California Housing\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing), dividido em treino, teste e validação.\n",
    "O modelo que você utilizará para aproximar a relação funcional entre as features e as labels é o modelo linear, i.e., $\\boldsymbol{y} = X\\theta$.\n",
    "Entretanto, você deve estimar seus parâmetros (minimizando o *mean squared error*) com **dois algoritmos diferentes**.\n",
    "Uma implementação deve estimar $\\theta$ por meio de **Stochastic Gradient Descent (SGD)** e, a outra, por meio de **Ordinary Least Squares (OLS)**, ou seja, utilizar a solução em fórmula fechada vista em aula.\n",
    "\n",
    "Para o SGD, o ponto inicial deve ser escolhido aleatoriamente e o algoritmo deve parar quando a norma da diferença entre duas estimativas consecutivas de $\\theta$ for menor do que um $\\varepsilon > 0$ previamente especificado.\n",
    "Para o experimento a seguir, fixe $\\varepsilon$ em um valor pequeno (por exemplo, alguma potência de $1/10$) para a qual o algoritmo convirja no máximo em alguns minutos para uma solução com perda pequena.\n",
    "\n",
    "Para diferentes tamanhos de minibatch (por exemplo $\\{2^{j}: 1 \\leq j \\leq 7\\}$), plote um gráfico representando o valor da perda $ L(\\hat{\\theta}) = \\frac{1}{n} \\lVert X \\hat{\\theta} - \\mathbf{y} \\rVert^{2}$ no conjunto de validação em função do número de epochs. Mostre também o valor ótimo obtido com OLS. Comente os resultados e o efeito tamanho do minibatch, e.g., no tempo de treinamento. Reporte valores nos conjuntos de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "s7ruzqo6EDZx",
    "ExecuteTime": {
     "end_time": "2024-03-12T14:05:52.257493Z",
     "start_time": "2024-03-12T14:05:52.244439Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "features, labels = fetch_california_housing(return_X_y=True)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.25\n",
    ")\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(\n",
    "    features_train, labels_train, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aERbmmnqDlYc",
    "ExecuteTime": {
     "end_time": "2024-03-12T14:05:53.005800Z",
     "start_time": "2024-03-12T14:05:52.259139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 9.458591517235775e+25\n",
      "iteration: 1, loss: 1.2820281097114428e+28\n",
      "iteration: 2, loss: 9.458591517235775e+25\n",
      "iteration: 3, loss: 1.2820281097114428e+28\n",
      "iteration: 4, loss: 9.458591517235775e+25\n",
      "iteration: 5, loss: 1.2820281097114428e+28\n",
      "iteration: 6, loss: 9.458591517235775e+25\n",
      "iteration: 7, loss: 1.2820281097114428e+28\n",
      "iteration: 8, loss: 9.458591517235775e+25\n",
      "iteration: 9, loss: 1.2820281097114428e+28\n",
      "iteration: 10, loss: 9.458591517235775e+25\n",
      "iteration: 11, loss: 1.2820281097114428e+28\n",
      "iteration: 12, loss: 9.458591517235775e+25\n",
      "iteration: 13, loss: 1.2820281097114428e+28\n",
      "iteration: 14, loss: 9.458591517235775e+25\n",
      "iteration: 15, loss: 1.2820281097114428e+28\n",
      "iteration: 16, loss: 9.458591517235775e+25\n",
      "iteration: 17, loss: 1.2820281097114428e+28\n",
      "iteration: 18, loss: 9.458591517235775e+25\n",
      "iteration: 19, loss: 1.2820281097114428e+28\n",
      "iteration: 20, loss: 9.458591517235775e+25\n",
      "iteration: 21, loss: 1.2820281097114428e+28\n",
      "iteration: 22, loss: 9.458591517235775e+25\n",
      "iteration: 23, loss: 1.2820281097114428e+28\n",
      "iteration: 24, loss: 9.458591517235775e+25\n",
      "iteration: 25, loss: 1.2820281097114428e+28\n",
      "iteration: 26, loss: 9.458591517235775e+25\n",
      "iteration: 27, loss: 1.2820281097114428e+28\n",
      "iteration: 28, loss: 9.458591517235775e+25\n",
      "iteration: 29, loss: 1.2820281097114428e+28\n",
      "iteration: 30, loss: 9.458591517235775e+25\n",
      "iteration: 31, loss: 1.2820281097114428e+28\n",
      "iteration: 32, loss: 9.458591517235775e+25\n",
      "iteration: 33, loss: 1.2820281097114428e+28\n",
      "iteration: 34, loss: 9.458591517235775e+25\n",
      "iteration: 35, loss: 1.2820281097114428e+28\n",
      "iteration: 36, loss: 9.458591517235775e+25\n",
      "iteration: 37, loss: 1.2820281097114428e+28\n",
      "iteration: 38, loss: 9.458591517235775e+25\n",
      "iteration: 39, loss: 1.2820281097114428e+28\n",
      "iteration: 40, loss: 9.458591517235775e+25\n",
      "iteration: 41, loss: 1.2820281097114428e+28\n",
      "iteration: 42, loss: 9.458591517235775e+25\n",
      "iteration: 43, loss: 1.2820281097114428e+28\n",
      "iteration: 44, loss: 9.458591517235775e+25\n",
      "iteration: 45, loss: 1.2820281097114428e+28\n",
      "iteration: 46, loss: 9.458591517235775e+25\n",
      "iteration: 47, loss: 1.2820281097114428e+28\n",
      "iteration: 48, loss: 9.458591517235775e+25\n",
      "iteration: 49, loss: 1.2820281097114428e+28\n",
      "iteration: 50, loss: 9.458591517235775e+25\n",
      "iteration: 51, loss: 1.2820281097114428e+28\n",
      "iteration: 52, loss: 9.458591517235775e+25\n",
      "iteration: 53, loss: 1.2820281097114428e+28\n",
      "iteration: 54, loss: 9.458591517235775e+25\n",
      "iteration: 55, loss: 1.2820281097114428e+28\n",
      "iteration: 56, loss: 9.458591517235775e+25\n",
      "iteration: 57, loss: 1.2820281097114428e+28\n",
      "iteration: 58, loss: 9.458591517235775e+25\n",
      "iteration: 59, loss: 1.2820281097114428e+28\n",
      "iteration: 60, loss: 9.458591517235775e+25\n",
      "iteration: 61, loss: 1.2820281097114428e+28\n",
      "iteration: 62, loss: 9.458591517235775e+25\n",
      "iteration: 63, loss: 1.2820281097114428e+28\n",
      "iteration: 64, loss: 9.458591517235775e+25\n",
      "iteration: 65, loss: 1.2820281097114428e+28\n",
      "iteration: 66, loss: 9.458591517235775e+25\n",
      "iteration: 67, loss: 1.2820281097114428e+28\n",
      "iteration: 68, loss: 9.458591517235775e+25\n",
      "iteration: 69, loss: 1.2820281097114428e+28\n",
      "iteration: 70, loss: 9.458591517235775e+25\n",
      "iteration: 71, loss: 1.2820281097114428e+28\n",
      "iteration: 72, loss: 9.458591517235775e+25\n",
      "iteration: 73, loss: 1.2820281097114428e+28\n",
      "iteration: 74, loss: 9.458591517235775e+25\n",
      "iteration: 75, loss: 1.2820281097114428e+28\n",
      "iteration: 76, loss: 9.458591517235775e+25\n",
      "iteration: 77, loss: 1.2820281097114428e+28\n",
      "iteration: 78, loss: 9.458591517235775e+25\n",
      "iteration: 79, loss: 1.2820281097114428e+28\n",
      "iteration: 80, loss: 9.458591517235775e+25\n",
      "iteration: 81, loss: 1.2820281097114428e+28\n",
      "iteration: 82, loss: 9.458591517235775e+25\n",
      "iteration: 83, loss: 1.2820281097114428e+28\n",
      "iteration: 84, loss: 9.458591517235775e+25\n",
      "iteration: 85, loss: 1.2820281097114428e+28\n",
      "iteration: 86, loss: 9.458591517235775e+25\n",
      "iteration: 87, loss: 1.2820281097114428e+28\n",
      "iteration: 88, loss: 9.458591517235775e+25\n",
      "iteration: 89, loss: 1.2820281097114428e+28\n",
      "iteration: 90, loss: 9.458591517235775e+25\n",
      "iteration: 91, loss: 1.2820281097114428e+28\n",
      "iteration: 92, loss: 9.458591517235775e+25\n",
      "iteration: 93, loss: 1.2820281097114428e+28\n",
      "iteration: 94, loss: 9.458591517235775e+25\n",
      "iteration: 95, loss: 1.2820281097114428e+28\n",
      "iteration: 96, loss: 9.458591517235775e+25\n",
      "iteration: 97, loss: 1.2820281097114428e+28\n",
      "iteration: 98, loss: 9.458591517235775e+25\n",
      "iteration: 99, loss: 1.2820281097114428e+28\n",
      "iteration: 100, loss: 9.458591517235775e+25\n",
      "iteration: 101, loss: 1.2820281097114428e+28\n",
      "iteration: 102, loss: 9.458591517235775e+25\n",
      "iteration: 103, loss: 1.2820281097114428e+28\n",
      "iteration: 104, loss: 9.458591517235775e+25\n",
      "iteration: 105, loss: 1.2820281097114428e+28\n",
      "iteration: 106, loss: 9.458591517235775e+25\n",
      "iteration: 107, loss: 1.2820281097114428e+28\n",
      "iteration: 108, loss: 9.458591517235775e+25\n",
      "iteration: 109, loss: 1.2820281097114428e+28\n",
      "iteration: 110, loss: 9.458591517235775e+25\n",
      "iteration: 111, loss: 1.2820281097114428e+28\n",
      "iteration: 112, loss: 9.458591517235775e+25\n",
      "iteration: 113, loss: 1.2820281097114428e+28\n",
      "iteration: 114, loss: 9.458591517235775e+25\n",
      "iteration: 115, loss: 1.2820281097114428e+28\n",
      "iteration: 116, loss: 9.458591517235775e+25\n",
      "iteration: 117, loss: 1.2820281097114428e+28\n",
      "iteration: 118, loss: 9.458591517235775e+25\n",
      "iteration: 119, loss: 1.2820281097114428e+28\n",
      "iteration: 120, loss: 9.458591517235775e+25\n",
      "iteration: 121, loss: 1.2820281097114428e+28\n",
      "iteration: 122, loss: 9.458591517235775e+25\n",
      "iteration: 123, loss: 1.2820281097114428e+28\n",
      "iteration: 124, loss: 9.458591517235775e+25\n",
      "iteration: 125, loss: 1.2820281097114428e+28\n",
      "iteration: 126, loss: 9.458591517235775e+25\n",
      "iteration: 127, loss: 1.2820281097114428e+28\n",
      "iteration: 128, loss: 9.458591517235775e+25\n",
      "iteration: 129, loss: 1.2820281097114428e+28\n",
      "iteration: 130, loss: 9.458591517235775e+25\n",
      "iteration: 131, loss: 1.2820281097114428e+28\n",
      "iteration: 132, loss: 9.458591517235775e+25\n",
      "iteration: 133, loss: 1.2820281097114428e+28\n",
      "iteration: 134, loss: 9.458591517235775e+25\n",
      "iteration: 135, loss: 1.2820281097114428e+28\n",
      "iteration: 136, loss: 9.458591517235775e+25\n",
      "iteration: 137, loss: 1.2820281097114428e+28\n",
      "iteration: 138, loss: 9.458591517235775e+25\n",
      "iteration: 139, loss: 1.2820281097114428e+28\n",
      "iteration: 140, loss: 9.458591517235775e+25\n",
      "iteration: 141, loss: 1.2820281097114428e+28\n",
      "iteration: 142, loss: 9.458591517235775e+25\n",
      "iteration: 143, loss: 1.2820281097114428e+28\n",
      "iteration: 144, loss: 9.458591517235775e+25\n",
      "iteration: 145, loss: 1.2820281097114428e+28\n",
      "iteration: 146, loss: 9.458591517235775e+25\n",
      "iteration: 147, loss: 1.2820281097114428e+28\n",
      "iteration: 148, loss: 9.458591517235775e+25\n",
      "iteration: 149, loss: 1.2820281097114428e+28\n",
      "iteration: 150, loss: 9.458591517235775e+25\n",
      "iteration: 151, loss: 1.2820281097114428e+28\n",
      "iteration: 152, loss: 9.458591517235775e+25\n",
      "iteration: 153, loss: 1.2820281097114428e+28\n",
      "iteration: 154, loss: 9.458591517235775e+25\n",
      "iteration: 155, loss: 1.2820281097114428e+28\n",
      "iteration: 156, loss: 9.458591517235775e+25\n",
      "iteration: 157, loss: 1.2820281097114428e+28\n",
      "iteration: 158, loss: 9.458591517235775e+25\n",
      "iteration: 159, loss: 1.2820281097114428e+28\n",
      "iteration: 160, loss: 9.458591517235775e+25\n",
      "iteration: 161, loss: 1.2820281097114428e+28\n",
      "iteration: 162, loss: 9.458591517235775e+25\n",
      "iteration: 163, loss: 1.2820281097114428e+28\n",
      "iteration: 164, loss: 9.458591517235775e+25\n",
      "iteration: 165, loss: 1.2820281097114428e+28\n",
      "iteration: 166, loss: 9.458591517235775e+25\n",
      "iteration: 167, loss: 1.2820281097114428e+28\n",
      "iteration: 168, loss: 9.458591517235775e+25\n",
      "iteration: 169, loss: 1.2820281097114428e+28\n",
      "iteration: 170, loss: 9.458591517235775e+25\n",
      "iteration: 171, loss: 1.2820281097114428e+28\n",
      "iteration: 172, loss: 9.458591517235775e+25\n",
      "iteration: 173, loss: 1.2820281097114428e+28\n",
      "iteration: 174, loss: 9.458591517235775e+25\n",
      "iteration: 175, loss: 1.2820281097114428e+28\n",
      "iteration: 176, loss: 9.458591517235775e+25\n",
      "iteration: 177, loss: 1.2820281097114428e+28\n",
      "iteration: 178, loss: 9.458591517235775e+25\n",
      "iteration: 179, loss: 1.2820281097114428e+28\n",
      "iteration: 180, loss: 9.458591517235775e+25\n",
      "iteration: 181, loss: 1.2820281097114428e+28\n",
      "iteration: 182, loss: 9.458591517235775e+25\n",
      "iteration: 183, loss: 1.2820281097114428e+28\n",
      "iteration: 184, loss: 9.458591517235775e+25\n",
      "iteration: 185, loss: 1.2820281097114428e+28\n",
      "iteration: 186, loss: 9.458591517235775e+25\n",
      "iteration: 187, loss: 1.2820281097114428e+28\n",
      "iteration: 188, loss: 9.458591517235775e+25\n",
      "iteration: 189, loss: 1.2820281097114428e+28\n",
      "iteration: 190, loss: 9.458591517235775e+25\n",
      "iteration: 191, loss: 1.2820281097114428e+28\n",
      "iteration: 192, loss: 9.458591517235775e+25\n",
      "iteration: 193, loss: 1.2820281097114428e+28\n",
      "iteration: 194, loss: 9.458591517235775e+25\n",
      "iteration: 195, loss: 1.2820281097114428e+28\n",
      "iteration: 196, loss: 9.458591517235775e+25\n",
      "iteration: 197, loss: 1.2820281097114428e+28\n",
      "iteration: 198, loss: 9.458591517235775e+25\n",
      "iteration: 199, loss: 1.2820281097114428e+28\n",
      "iteration: 200, loss: 9.458591517235775e+25\n",
      "iteration: 201, loss: 1.2820281097114428e+28\n",
      "iteration: 202, loss: 9.458591517235775e+25\n",
      "iteration: 203, loss: 1.2820281097114428e+28\n",
      "iteration: 204, loss: 9.458591517235775e+25\n",
      "iteration: 205, loss: 1.2820281097114428e+28\n",
      "iteration: 206, loss: 9.458591517235775e+25\n",
      "iteration: 207, loss: 1.2820281097114428e+28\n",
      "iteration: 208, loss: 9.458591517235775e+25\n",
      "iteration: 209, loss: 1.2820281097114428e+28\n",
      "iteration: 210, loss: 9.458591517235775e+25\n",
      "iteration: 211, loss: 1.2820281097114428e+28\n",
      "iteration: 212, loss: 9.458591517235775e+25\n",
      "iteration: 213, loss: 1.2820281097114428e+28\n",
      "iteration: 214, loss: 9.458591517235775e+25\n",
      "iteration: 215, loss: 1.2820281097114428e+28\n",
      "iteration: 216, loss: 9.458591517235775e+25\n",
      "iteration: 217, loss: 1.2820281097114428e+28\n",
      "iteration: 218, loss: 9.458591517235775e+25\n",
      "iteration: 219, loss: 1.2820281097114428e+28\n",
      "iteration: 220, loss: 9.458591517235775e+25\n",
      "iteration: 221, loss: 1.2820281097114428e+28\n",
      "iteration: 222, loss: 9.458591517235775e+25\n",
      "iteration: 223, loss: 1.2820281097114428e+28\n",
      "iteration: 224, loss: 9.458591517235775e+25\n",
      "iteration: 225, loss: 1.2820281097114428e+28\n",
      "iteration: 226, loss: 9.458591517235775e+25\n",
      "iteration: 227, loss: 1.2820281097114428e+28\n",
      "iteration: 228, loss: 9.458591517235775e+25\n",
      "iteration: 229, loss: 1.2820281097114428e+28\n",
      "iteration: 230, loss: 9.458591517235775e+25\n",
      "iteration: 231, loss: 1.2820281097114428e+28\n",
      "iteration: 232, loss: 9.458591517235775e+25\n",
      "iteration: 233, loss: 1.2820281097114428e+28\n",
      "iteration: 234, loss: 9.458591517235775e+25\n",
      "iteration: 235, loss: 1.2820281097114428e+28\n",
      "iteration: 236, loss: 9.458591517235775e+25\n",
      "iteration: 237, loss: 1.2820281097114428e+28\n",
      "iteration: 238, loss: 9.458591517235775e+25\n",
      "iteration: 239, loss: 1.2820281097114428e+28\n",
      "iteration: 240, loss: 9.458591517235775e+25\n",
      "iteration: 241, loss: 1.2820281097114428e+28\n",
      "iteration: 242, loss: 9.458591517235775e+25\n",
      "iteration: 243, loss: 1.2820281097114428e+28\n",
      "iteration: 244, loss: 9.458591517235775e+25\n",
      "iteration: 245, loss: 1.2820281097114428e+28\n",
      "iteration: 246, loss: 9.458591517235775e+25\n",
      "iteration: 247, loss: 1.2820281097114428e+28\n",
      "iteration: 248, loss: 9.458591517235775e+25\n",
      "iteration: 249, loss: 1.2820281097114428e+28\n",
      "iteration: 250, loss: 9.458591517235775e+25\n",
      "iteration: 251, loss: 1.2820281097114428e+28\n",
      "iteration: 252, loss: 9.458591517235775e+25\n",
      "iteration: 253, loss: 1.2820281097114428e+28\n",
      "iteration: 254, loss: 9.458591517235775e+25\n",
      "iteration: 255, loss: 1.2820281097114428e+28\n",
      "iteration: 256, loss: 9.458591517235775e+25\n",
      "iteration: 257, loss: 1.2820281097114428e+28\n",
      "iteration: 258, loss: 9.458591517235775e+25\n",
      "iteration: 259, loss: 1.2820281097114428e+28\n",
      "iteration: 260, loss: 9.458591517235775e+25\n",
      "iteration: 261, loss: 1.2820281097114428e+28\n",
      "iteration: 262, loss: 9.458591517235775e+25\n",
      "iteration: 263, loss: 1.2820281097114428e+28\n",
      "iteration: 264, loss: 9.458591517235775e+25\n",
      "iteration: 265, loss: 1.2820281097114428e+28\n",
      "iteration: 266, loss: 9.458591517235775e+25\n",
      "iteration: 267, loss: 1.2820281097114428e+28\n",
      "iteration: 268, loss: 9.458591517235775e+25\n",
      "iteration: 269, loss: 1.2820281097114428e+28\n",
      "iteration: 270, loss: 9.458591517235775e+25\n",
      "iteration: 271, loss: 1.2820281097114428e+28\n",
      "iteration: 272, loss: 9.458591517235775e+25\n",
      "iteration: 273, loss: 1.2820281097114428e+28\n",
      "iteration: 274, loss: 9.458591517235775e+25\n",
      "iteration: 275, loss: 1.2820281097114428e+28\n",
      "iteration: 276, loss: 9.458591517235775e+25\n",
      "iteration: 277, loss: 1.2820281097114428e+28\n",
      "iteration: 278, loss: 9.458591517235775e+25\n",
      "iteration: 279, loss: 1.2820281097114428e+28\n",
      "iteration: 280, loss: 9.458591517235775e+25\n",
      "iteration: 281, loss: 1.2820281097114428e+28\n",
      "iteration: 282, loss: 9.458591517235775e+25\n",
      "iteration: 283, loss: 1.2820281097114428e+28\n",
      "iteration: 284, loss: 9.458591517235775e+25\n",
      "iteration: 285, loss: 1.2820281097114428e+28\n",
      "iteration: 286, loss: 9.458591517235775e+25\n",
      "iteration: 287, loss: 1.2820281097114428e+28\n",
      "iteration: 288, loss: 9.458591517235775e+25\n",
      "iteration: 289, loss: 1.2820281097114428e+28\n",
      "iteration: 290, loss: 9.458591517235775e+25\n",
      "iteration: 291, loss: 1.2820281097114428e+28\n",
      "iteration: 292, loss: 9.458591517235775e+25\n",
      "iteration: 293, loss: 1.2820281097114428e+28\n",
      "iteration: 294, loss: 9.458591517235775e+25\n",
      "iteration: 295, loss: 1.2820281097114428e+28\n",
      "iteration: 296, loss: 9.458591517235775e+25\n",
      "iteration: 297, loss: 1.2820281097114428e+28\n",
      "iteration: 298, loss: 9.458591517235775e+25\n",
      "iteration: 299, loss: 1.2820281097114428e+28\n",
      "iteration: 300, loss: 9.458591517235775e+25\n",
      "iteration: 301, loss: 1.2820281097114428e+28\n",
      "iteration: 302, loss: 9.458591517235775e+25\n",
      "iteration: 303, loss: 1.2820281097114428e+28\n",
      "iteration: 304, loss: 9.458591517235775e+25\n",
      "iteration: 305, loss: 1.2820281097114428e+28\n",
      "iteration: 306, loss: 9.458591517235775e+25\n",
      "iteration: 307, loss: 1.2820281097114428e+28\n",
      "iteration: 308, loss: 9.458591517235775e+25\n",
      "iteration: 309, loss: 1.2820281097114428e+28\n",
      "iteration: 310, loss: 9.458591517235775e+25\n",
      "iteration: 311, loss: 1.2820281097114428e+28\n",
      "iteration: 312, loss: 9.458591517235775e+25\n",
      "iteration: 313, loss: 1.2820281097114428e+28\n",
      "iteration: 314, loss: 9.458591517235775e+25\n",
      "iteration: 315, loss: 1.2820281097114428e+28\n",
      "iteration: 316, loss: 9.458591517235775e+25\n",
      "iteration: 317, loss: 1.2820281097114428e+28\n",
      "iteration: 318, loss: 9.458591517235775e+25\n",
      "iteration: 319, loss: 1.2820281097114428e+28\n",
      "iteration: 320, loss: 9.458591517235775e+25\n",
      "iteration: 321, loss: 1.2820281097114428e+28\n",
      "iteration: 322, loss: 9.458591517235775e+25\n",
      "iteration: 323, loss: 1.2820281097114428e+28\n",
      "iteration: 324, loss: 9.458591517235775e+25\n",
      "iteration: 325, loss: 1.2820281097114428e+28\n",
      "iteration: 326, loss: 9.458591517235775e+25\n",
      "iteration: 327, loss: 1.2820281097114428e+28\n",
      "iteration: 328, loss: 9.458591517235775e+25\n",
      "iteration: 329, loss: 1.2820281097114428e+28\n",
      "iteration: 330, loss: 9.458591517235775e+25\n",
      "iteration: 331, loss: 1.2820281097114428e+28\n",
      "iteration: 332, loss: 9.458591517235775e+25\n",
      "iteration: 333, loss: 1.2820281097114428e+28\n",
      "iteration: 334, loss: 9.458591517235775e+25\n",
      "iteration: 335, loss: 1.2820281097114428e+28\n",
      "iteration: 336, loss: 9.458591517235775e+25\n",
      "iteration: 337, loss: 1.2820281097114428e+28\n",
      "iteration: 338, loss: 9.458591517235775e+25\n",
      "iteration: 339, loss: 1.2820281097114428e+28\n",
      "iteration: 340, loss: 9.458591517235775e+25\n",
      "iteration: 341, loss: 1.2820281097114428e+28\n",
      "iteration: 342, loss: 9.458591517235775e+25\n",
      "iteration: 343, loss: 1.2820281097114428e+28\n",
      "iteration: 344, loss: 9.458591517235775e+25\n",
      "iteration: 345, loss: 1.2820281097114428e+28\n",
      "iteration: 346, loss: 9.458591517235775e+25\n",
      "iteration: 347, loss: 1.2820281097114428e+28\n",
      "iteration: 348, loss: 9.458591517235775e+25\n",
      "iteration: 349, loss: 1.2820281097114428e+28\n",
      "iteration: 350, loss: 9.458591517235775e+25\n",
      "iteration: 351, loss: 1.2820281097114428e+28\n",
      "iteration: 352, loss: 9.458591517235775e+25\n",
      "iteration: 353, loss: 1.2820281097114428e+28\n",
      "iteration: 354, loss: 9.458591517235775e+25\n",
      "iteration: 355, loss: 1.2820281097114428e+28\n",
      "iteration: 356, loss: 9.458591517235775e+25\n",
      "iteration: 357, loss: 1.2820281097114428e+28\n",
      "iteration: 358, loss: 9.458591517235775e+25\n",
      "iteration: 359, loss: 1.2820281097114428e+28\n",
      "iteration: 360, loss: 9.458591517235775e+25\n",
      "iteration: 361, loss: 1.2820281097114428e+28\n",
      "iteration: 362, loss: 9.458591517235775e+25\n",
      "iteration: 363, loss: 1.2820281097114428e+28\n",
      "iteration: 364, loss: 9.458591517235775e+25\n",
      "iteration: 365, loss: 1.2820281097114428e+28\n",
      "iteration: 366, loss: 9.458591517235775e+25\n",
      "iteration: 367, loss: 1.2820281097114428e+28\n",
      "iteration: 368, loss: 9.458591517235775e+25\n",
      "iteration: 369, loss: 1.2820281097114428e+28\n",
      "iteration: 370, loss: 9.458591517235775e+25\n",
      "iteration: 371, loss: 1.2820281097114428e+28\n",
      "iteration: 372, loss: 9.458591517235775e+25\n",
      "iteration: 373, loss: 1.2820281097114428e+28\n",
      "iteration: 374, loss: 9.458591517235775e+25\n",
      "iteration: 375, loss: 1.2820281097114428e+28\n",
      "iteration: 376, loss: 9.458591517235775e+25\n",
      "iteration: 377, loss: 1.2820281097114428e+28\n",
      "iteration: 378, loss: 9.458591517235775e+25\n",
      "iteration: 379, loss: 1.2820281097114428e+28\n",
      "iteration: 380, loss: 9.458591517235775e+25\n",
      "iteration: 381, loss: 1.2820281097114428e+28\n",
      "iteration: 382, loss: 9.458591517235775e+25\n",
      "iteration: 383, loss: 1.2820281097114428e+28\n",
      "iteration: 384, loss: 9.458591517235775e+25\n",
      "iteration: 385, loss: 1.2820281097114428e+28\n",
      "iteration: 386, loss: 9.458591517235775e+25\n",
      "iteration: 387, loss: 1.2820281097114428e+28\n",
      "iteration: 388, loss: 9.458591517235775e+25\n",
      "iteration: 389, loss: 1.2820281097114428e+28\n",
      "iteration: 390, loss: 9.458591517235775e+25\n",
      "iteration: 391, loss: 1.2820281097114428e+28\n",
      "iteration: 392, loss: 9.458591517235775e+25\n",
      "iteration: 393, loss: 1.2820281097114428e+28\n",
      "iteration: 394, loss: 9.458591517235775e+25\n",
      "iteration: 395, loss: 1.2820281097114428e+28\n",
      "iteration: 396, loss: 9.458591517235775e+25\n",
      "iteration: 397, loss: 1.2820281097114428e+28\n",
      "iteration: 398, loss: 9.458591517235775e+25\n",
      "iteration: 399, loss: 1.2820281097114428e+28\n",
      "iteration: 400, loss: 9.458591517235775e+25\n",
      "iteration: 401, loss: 1.2820281097114428e+28\n",
      "iteration: 402, loss: 9.458591517235775e+25\n",
      "iteration: 403, loss: 1.2820281097114428e+28\n",
      "iteration: 404, loss: 9.458591517235775e+25\n",
      "iteration: 405, loss: 1.2820281097114428e+28\n",
      "iteration: 406, loss: 9.458591517235775e+25\n",
      "iteration: 407, loss: 1.2820281097114428e+28\n",
      "iteration: 408, loss: 9.458591517235775e+25\n",
      "iteration: 409, loss: 1.2820281097114428e+28\n",
      "iteration: 410, loss: 9.458591517235775e+25\n",
      "iteration: 411, loss: 1.2820281097114428e+28\n",
      "iteration: 412, loss: 9.458591517235775e+25\n",
      "iteration: 413, loss: 1.2820281097114428e+28\n",
      "iteration: 414, loss: 9.458591517235775e+25\n",
      "iteration: 415, loss: 1.2820281097114428e+28\n",
      "iteration: 416, loss: 9.458591517235775e+25\n",
      "iteration: 417, loss: 1.2820281097114428e+28\n",
      "iteration: 418, loss: 9.458591517235775e+25\n",
      "iteration: 419, loss: 1.2820281097114428e+28\n",
      "iteration: 420, loss: 9.458591517235775e+25\n",
      "iteration: 421, loss: 1.2820281097114428e+28\n",
      "iteration: 422, loss: 9.458591517235775e+25\n",
      "iteration: 423, loss: 1.2820281097114428e+28\n",
      "iteration: 424, loss: 9.458591517235775e+25\n",
      "iteration: 425, loss: 1.2820281097114428e+28\n",
      "iteration: 426, loss: 9.458591517235775e+25\n",
      "iteration: 427, loss: 1.2820281097114428e+28\n",
      "iteration: 428, loss: 9.458591517235775e+25\n",
      "iteration: 429, loss: 1.2820281097114428e+28\n",
      "iteration: 430, loss: 9.458591517235775e+25\n",
      "iteration: 431, loss: 1.2820281097114428e+28\n",
      "iteration: 432, loss: 9.458591517235775e+25\n",
      "iteration: 433, loss: 1.2820281097114428e+28\n",
      "iteration: 434, loss: 9.458591517235775e+25\n",
      "iteration: 435, loss: 1.2820281097114428e+28\n",
      "iteration: 436, loss: 9.458591517235775e+25\n",
      "iteration: 437, loss: 1.2820281097114428e+28\n",
      "iteration: 438, loss: 9.458591517235775e+25\n",
      "iteration: 439, loss: 1.2820281097114428e+28\n",
      "iteration: 440, loss: 9.458591517235775e+25\n",
      "iteration: 441, loss: 1.2820281097114428e+28\n",
      "iteration: 442, loss: 9.458591517235775e+25\n",
      "iteration: 443, loss: 1.2820281097114428e+28\n",
      "iteration: 444, loss: 9.458591517235775e+25\n",
      "iteration: 445, loss: 1.2820281097114428e+28\n",
      "iteration: 446, loss: 9.458591517235775e+25\n",
      "iteration: 447, loss: 1.2820281097114428e+28\n",
      "iteration: 448, loss: 9.458591517235775e+25\n",
      "iteration: 449, loss: 1.2820281097114428e+28\n",
      "iteration: 450, loss: 9.458591517235775e+25\n",
      "iteration: 451, loss: 1.2820281097114428e+28\n",
      "iteration: 452, loss: 9.458591517235775e+25\n",
      "iteration: 453, loss: 1.2820281097114428e+28\n",
      "iteration: 454, loss: 9.458591517235775e+25\n",
      "iteration: 455, loss: 1.2820281097114428e+28\n",
      "iteration: 456, loss: 9.458591517235775e+25\n",
      "iteration: 457, loss: 1.2820281097114428e+28\n",
      "iteration: 458, loss: 9.458591517235775e+25\n",
      "iteration: 459, loss: 1.2820281097114428e+28\n",
      "iteration: 460, loss: 9.458591517235775e+25\n",
      "iteration: 461, loss: 1.2820281097114428e+28\n",
      "iteration: 462, loss: 9.458591517235775e+25\n",
      "iteration: 463, loss: 1.2820281097114428e+28\n",
      "iteration: 464, loss: 9.458591517235775e+25\n",
      "iteration: 465, loss: 1.2820281097114428e+28\n",
      "iteration: 466, loss: 9.458591517235775e+25\n",
      "iteration: 467, loss: 1.2820281097114428e+28\n",
      "iteration: 468, loss: 9.458591517235775e+25\n",
      "iteration: 469, loss: 1.2820281097114428e+28\n",
      "iteration: 470, loss: 9.458591517235775e+25\n",
      "iteration: 471, loss: 1.2820281097114428e+28\n",
      "iteration: 472, loss: 9.458591517235775e+25\n",
      "iteration: 473, loss: 1.2820281097114428e+28\n",
      "iteration: 474, loss: 9.458591517235775e+25\n",
      "iteration: 475, loss: 1.2820281097114428e+28\n",
      "iteration: 476, loss: 9.458591517235775e+25\n",
      "iteration: 477, loss: 1.2820281097114428e+28\n",
      "iteration: 478, loss: 9.458591517235775e+25\n",
      "iteration: 479, loss: 1.2820281097114428e+28\n",
      "iteration: 480, loss: 9.458591517235775e+25\n",
      "iteration: 481, loss: 1.2820281097114428e+28\n",
      "iteration: 482, loss: 9.458591517235775e+25\n",
      "iteration: 483, loss: 1.2820281097114428e+28\n",
      "iteration: 484, loss: 9.458591517235775e+25\n",
      "iteration: 485, loss: 1.2820281097114428e+28\n",
      "iteration: 486, loss: 9.458591517235775e+25\n",
      "iteration: 487, loss: 1.2820281097114428e+28\n",
      "iteration: 488, loss: 9.458591517235775e+25\n",
      "iteration: 489, loss: 1.2820281097114428e+28\n",
      "iteration: 490, loss: 9.458591517235775e+25\n",
      "iteration: 491, loss: 1.2820281097114428e+28\n",
      "iteration: 492, loss: 9.458591517235775e+25\n",
      "iteration: 493, loss: 1.2820281097114428e+28\n",
      "iteration: 494, loss: 9.458591517235775e+25\n",
      "iteration: 495, loss: 1.2820281097114428e+28\n",
      "iteration: 496, loss: 9.458591517235775e+25\n",
      "iteration: 497, loss: 1.2820281097114428e+28\n",
      "iteration: 498, loss: 9.458591517235775e+25\n",
      "iteration: 499, loss: 1.2820281097114428e+28\n"
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "def linear_regression_PI(X, y):\n",
    "    \"\"\"\n",
    "    Implements a Linear Regression model using the Moore-Penrose pseudo-inverse\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        A 2-dimensional array with samples in the rows and features in the columns\n",
    "    y : array\n",
    "        An array with the same number of  as samples in X, the values to predict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array\n",
    "        Learnt parameters\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The first column of w corresponds to the bias (`w_0`)\n",
    "    \"\"\"\n",
    "    bias = np.ones((len(X),1))\n",
    "    X = np.concatenate((bias,X),axis=1)\n",
    "    pseudo_inverse = np.linalg.pinv(X)\n",
    "    w = np.matmul(pseudo_inverse,y)\n",
    "    return w\n",
    "\n",
    "\n",
    "def linear_regression_SGD(X_train, y_train, X_validation, y_validation, lr = 0.1, epsilon=(1/10)**5, batches = 50, max_iter=500,clip_value=1e+10):\n",
    "    \"\"\"\n",
    "    Implements a Linear Regression model using Stochastic Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        A 2-dimensional array with samples in the rows and features in the columns\n",
    "    y : array\n",
    "        An array with the same number of  as samples in X, the values to predict\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    max_iter : int\n",
    "        number of epochs to use for the gradient descent\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array\n",
    "        Learnt parameters\n",
    "    sse_history : array\n",
    "        An array that contains the error of the model in every iteration\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the gradient of the sum of squares function (Equations 3.12, and 3.23 in the Bishop book)\n",
    "    \"\"\"\n",
    "    bias = np.ones((len(X_train),1))\n",
    "    X_train = np.concatenate((bias,X_train),axis=1)\n",
    "    bias_validation = np.ones((len(X_validation),1))\n",
    "    X_validation = np.concatenate((bias_validation,X_validation),axis=1)\n",
    "    w=np.random.normal(0,1,X_train.shape[1])\n",
    "    sse_history = []\n",
    "    batch_size = X_train.shape[0]//batches\n",
    "    for iteration in range(max_iter):\n",
    "        for j in range(0,X_train.shape[0],batch_size):\n",
    "            error = y_train[j:j+batch_size]-np.matmul(X_train[j:j+batch_size], w)\n",
    "            gradient = -np.matmul(X_train[j:j+batch_size].T, error)\n",
    "            gradient = np.clip(gradient, -clip_value, clip_value)\n",
    "            w = w - lr * gradient\n",
    "        sse = np.sum((np.matmul(X_validation,w)-y_validation)**2)\n",
    "        sse_history.append(sse)\n",
    "        print(f\"iteration: {iteration}, loss: {sse}\")\n",
    "        if iteration > 0 and abs(sse_history[-1] - sse_history[-2]) < epsilon:\n",
    "            break\n",
    "    return w,sse_history\n",
    "\n",
    "\n",
    "w2 = linear_regression_PI(features_train, labels_train)\n",
    "w, sse_history = linear_regression_SGD(features_train, labels_train,features_validation,labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28,\n 9.458591517235775e+25,\n 1.2820281097114428e+28]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-12T14:05:53.019895Z",
     "start_time": "2024-03-12T14:05:53.007406Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXkMBbkNTd6i"
   },
   "source": [
    "**Exercício 2.** Agora, utilizando ainda o mesmo dataset da questão anterior, você deve implementar uma **Rede RBF** com função de base Gaussiana (veja as notas de aula).\n",
    "Para os centróides, utilize o output de um modelo de clusterização por K médias, por meio da função que disponibilizamos, como a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K82Fm4OTfGr",
    "outputId": "ce844ded-b104-4e1b-ca13-3f2ddb81cd57",
    "ExecuteTime": {
     "end_time": "2024-03-12T14:05:53.057056Z",
     "start_time": "2024-03-12T14:05:53.022138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  2.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "def k_means_factory(n_clusters: int) -> KMeans:\n",
    "    return KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "\n",
    "k_means_model = k_means_factory(n_clusters=2)\n",
    "dumb_data = np.array(\n",
    "    [[1, 2],\n",
    "     [1, 4],\n",
    "     [1, 0],\n",
    "     [10, 2],\n",
    "     [10, 4],\n",
    "     [10, 0]]\n",
    ")\n",
    "k_means_model.fit(dumb_data)\n",
    "cluster_centers = k_means_model.cluster_centers_\n",
    "print(cluster_centers) # Shape (n_clusters, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f9twmSXVq05"
   },
   "source": [
    "Para determinar o melhor valor de $k$ para o algoritmo de clusterização, treine o modelo (usando a fórmula de OLS) com diferentes valores e escolha o que possuir o menor erro de validação. Faça um gráfico mostrando o valor do erro de validação para diferentes valores de $k$. Mostre também a performance do modelo escolhido no conjunto de teste. Compare com o modelo linear simples da questão anterior. Discuta os resultados.\n",
    "\n",
    "Para definir o valor do hiper-parâmetro $\\gamma$, use a seguinte heurística --- que pode ser achado no livro \"Neural Networks\", por Simon Haykin:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{d_\\text{max}^2},\n",
    "$$\n",
    "\n",
    "onde $d_\\text{max}$ é a maior distância entre um par de centróides. Note que o valor costuma mudar para $k$'s diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:05:53.068757Z",
     "start_time": "2024-03-12T14:05:53.059625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvACUfsQWm11"
   },
   "source": [
    "# Exercícios de \"papel e caneta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 1.** Deixe que $X \\in \\mathbb{R}^{N\\times D}$, $c>0$ e $I$ denote a matriz identidade de dimensão $N$.\n",
    " Mostre que $X^\\intercal X + c I$ possui inversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 2.** Deixe que $X \\in \\mathbb{R}^{N\\times D}$ seja uma matriz contendo os exemplos de treinamento (um por linha) e que $y\\in \\mathbb{R}^N$ seja um vetor coluna dos outputs observados para cada vetor de input em suas linhas. Na aula, derivamos a solução de mínimos quadrados ordinários (OLS). Use o mesmo raciocínio para resolver achar o vetor de pesos ${\\theta}$ que minimiza:\n",
    " \n",
    "$$ \\|X \\theta - y\\|_2^2 + c \\|\\theta\\|_2^2,$$\n",
    "\n",
    "onde $c>0$ é uma constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 3.** Em algumas situações, temos muito mais features que amostras ($D \\gg N$). Esse tipo de cenário é comum, e.g., na análise de dados genômicos. Nesse caso, costumam existir infinitas combinações lineares das features que expressam o vetor de saídas $y$. Portanto, precisamos de algum critério para escolher um deles. Uma abordagem possível, é escolher o vetor de pesos $\\theta$ que possua menor norma L2.\n",
    "Com isso em mente, derive a solução que minimiza $\\|\\theta\\|_2^2$ e respeita $X \\theta = y$. Assuma que as linhas de $X$ são linearmente independentes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
